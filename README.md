<div style="text-align:center;">
  <img src="images/logo.png" width="500" />
</div>

<h1 align="center">Analisis de factores influyentes en la esperanza de vida al nacer</h1>

![](https://espaciomex.com/wp-content/uploads/2019/09/im_issuebrief__large.jpg)


Somos una consultora especializada en análisis  y toma de decisiones basada en datos. Nuestro equipo está conformado por profesionales altamente capacitados y con amplia experiencia en proyectos de consultoría de datos, lo que nos permite abordar de manera integral proyectos de diversa complejidad y magnitud.

Nuestra misión es ayudar a nuestros clientes a obtener información valiosa a partir de sus datos para mejorar su toma de decisiones y optimizar sus procesos. Nos destacamos por brindar soluciones personalizadas, adaptadas a las necesidades y objetivos específicos de cada cliente. Trabajamos de manera colaborativa, estableciendo una estrecha relación con nuestros clientes para comprender a fondo su problemática y ofrecerles las mejores soluciones.

---
## Entendimiento de la situación actual:
La empresa farmacéutica Pfizer ha delineado su objetivo de desarrollar un nuevo medicamento. Para dar inicio a la planificación de este proyecto, la empresa requiere recopilar datos relevantes que influyan en la esperanza de vida, tasas de fecundidad promedio y otros factores demográficos clave. Esta información será fundamental para determinar la ubicación demográfica más adecuada en la cual invertir y llevar a cabo el desarrollo del medicamento.
<div style="text-align:center;">
  <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Pfizer.svg/1000px-Pfizer.svg.png" width="200" />
</div>

Para lograr este objetivo, Pfizer ha contratado a nuestra consultora, con el fin de realizar un estudio sobre la esperanza de vida y otros indicadores relevantes para comprender los factores que influyen en la vitalidad de las personas.
Con el modelo de ML trataremos de resolver estos interrogantes
¿Qué hace que un país tenga mayor esperanza de vida?
¿Cuáles son las características predominantes para que un país tenga mayor esperanza de vida?

Para la realización de este proyecto utilizamos principalmente las siguientes fuentes.



Banco mundial: https://data.worldbank.org/

Organización mundial de la salud: https://www.who.int/

<div style="text-align:center;">
  <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/The_World_Bank_logo.svg/2560px-The_World_Bank_logo.svg.png" alt="The World Bank" width="200" />
  <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/World_Health_Organization_Logo.svg/2560px-World_Health_Organization_Logo.svg.png" alt="World Health Organization" width="200" />
</div>

---
## Objetivos:
1. Analizar las tendencias actuales de la esperanza de vida en diferentes países y regiones.
2. Identificar los factores determinantes de la esperanza de vida, como las tasas de mortalidad infantil, la calidad de los servicios de atención médica, el acceso a servicios básicos de salud y otros indicadores relevantes.
3. Proporcionar recomendaciones para el desarrollo del nuevo medicamento que promueve la fertilidad y la vitalidad en hombres y mujeres.

---

## Alcance:
Recopilar y analizar datos demográficos, como la esperanza de vida al nacer, esperanza de vida saludable, el índice de envejecimiento y otros, de diferentes fuentes confiables.
El estudio se centrará en países de América del Sur, América del Norte, Centroamérica y Oceanía, utilizando datos recopilados durante los últimos 30 años.
Como posibilidad de continuidad del proyecto, se plantea la expansión a otros países de los otros continentes o la inclusión de factores adicionales.

---
## KPIs:  
La metodología de determinación de KPI sigue el concepto de los reconocidos objetivos SMART.  El acrónimo S (specific) específico, M (Mesurable) medible, A (Attainable) alcanzable, R (Relevant) relevante, T (Time-bound) con limite de tiempo.

---
# LLENAR DAVID
`Esperanza de vida saludable` - KPI aumento 0.1% comparando dos últimos años con datos. Es una medida que combina la esperanza de vida al nacer con la proporción de años vividos en buena salud. El índice de esperanza de vida saludable proporciona una visión más completa de la salud de una población al considerar tanto la duración como la calidad de vida.  
`Mortalidad Infantil` - KPI disminución 0.1% tasa de mortalidad infantil comparando dos últimos años con datos. Es la cantidad de niños que mueren antes de cumplir un año de edad por cada 1,000 nacidos vivos. Es un KPI crítico para evaluar la calidad de los servicios de atención médica materno-infantil y la salud general de la población.
`Mortalidad materna`  
`Esperanza de vida al nacer` - KPI aumento 0.1% comparando dos últimos años con datos: Es el indicador principal que mide la esperanza de vida promedio de una población al nacer. Es un KPI fundamental para evaluar la salud y el bienestar general de una población.  
⦁	KPI: Disminución del 0,1% de la tasa de fecundidad en mujeres de edad 15 a 19 años.(verificar la data) 

⦁	KPI: Aumento del 0,1% de la tasa de fecundidad en mujeres mayores a 30 años(verificar la data)


---
## Solución propuesta:
`Análisis de datos:` Realizar un análisis exploratorio de los datos disponibles y limpiarlos para su posterior uso.  
`Modelado y análisis:` Desarrollar modelos estadísticos y de aprendizaje automático para comprender los factores que influyen en la esperanza de vida.  
`Dashboard de visualización:` Crear un dashboard interactivo que presente los resultados del análisis y permita explorar los datos.  
`Informe y recomendaciones:` Documentar los hallazgos del análisis y proporcionar recomendaciones basadas en los resultados.

---
## Stack tecnológico:
`Herramientas de trabajo:` Visual Studio Code, Jupyter Lab, Google Colab y Amazon Wb service(AWS).  
<p align="center">
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Visual_Studio_Code_1.35_icon.svg/2048px-Visual_Studio_Code_1.35_icon.svg.png" width="200" />
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Jupyter_logo.svg/1767px-Jupyter_logo.svg.png" width="200" />
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Wikimedia-Collab-logo.svg/2048px-Wikimedia-Collab-logo.svg.png" width="200" />
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Amazon_Web_Services_Logo.svg/2560px-Amazon_Web_Services_Logo.svg.png" width="200" />
</p>    

`Lenguaje de programación:` Python.  
<p align="center">
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/1869px-Python-logo-notext.svg.png" width="200" />
</p>

`Bibliotecas:` Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn, y Pyspark.  
<p align="center">
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/NumPy_logo.svg/2560px-NumPy_logo.svg.png" width="200" />
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Pandas_logo.svg/2560px-Pandas_logo.svg.png" width="200" />
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Matplotlib_icon.svg/2048px-Matplotlib_icon.svg.png" width="200" />
    <img src="https://upload.wikimedia.org/wikipedia/commons/e/ea/Spark-logo-192x100px.png" width="200" />
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Scikit_learn_logo_small.svg/2560px-Scikit_learn_logo_small.svg.png" width="200" />
</p>

`Herramientas de visualización:` Power BI, Streamlit 
<p align="center">
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/New_Power_BI_Logo.svg/2048px-New_Power_BI_Logo.svg.png" width="200" />
    <img src="https://upload.wikimedia.org/wikipedia/commons/7/77/Streamlit-logo-primary-colormark-darktext.png" width="200" />
</p>    

`Repositorio de código:` GitHub    
<p align="center">
    <img src="https://upload.wikimedia.org/wikipedia/commons/4/4a/GitHub_Mark.png" width="200" />
</p>    

---
## Diseño detallado 
`Análisis exploratorio de datos:` Documento que detalla el proceso de limpieza y exploración de los datos.  
`Modelos estadísticos y de aprendizaje automático:` Informe que describe los modelos desarrollados y los resultados obtenidos.  
`Dashboard interactivo:` Presentación visual de los resultados del análisis, incluyendo gráficos y visualizaciones interactivas.  
`Informe final y recomendaciones:` Documento que resume los hallazgos del análisis y proporciona recomendaciones basadas en los resultados.  

---
## Equipo de trabajo - Roles y responsabilidades:
`Data Engineer:` Encargado de diseñar e implementar los pipelines de ETL y configurar la infraestructura de datos.  
`Data Analyst:` Responsable del análisis exploratorio de datos, modelado estadístico y desarrollo de modelos predictivos.  
`Data Visualizer:` Encargado de crear visualizaciones y el dashboard interactivo para presentar los resultados del análisis.  
`Functional Analyst:` Asegurar de que el proyecto progrese de manera efectiva y se alcancen los objetivos establecidos al asumir diferentes roles.  
# creo que no iria en el README
⦁	Cronograma general - Gantt:
Se creará un diagrama de Gantt que muestre las tareas y los plazos estimados para cada una de las etapas del proyecto. Esto permitirá un seguimiento claro del progreso y asegurará el cumplimiento de los hitos establecidos.[https://app.teamgantt.com/projects/gantt?ids=3553277]
 
---
## Análisis preliminar de calidad de datos:
Realizaremos un análisis detallado de los datos con los que vamos a trabajar. Analizaremos  la descripción de cada indicador, los tipos de datos, el método de adquisición y las fechas de adquisición y actualización.
Para la recopilación y el análisis de los datos, decidimos basarnos en la base de datos brindada por el Banco Mundial y la Organización Mundial de la Salud (WHO) dado que la confianza y transparencia de su información.

---
## Variedad de fuentes de datos:

Al utilizar tanto la descarga de archivos CSV del Banco Mundial como la API de la OMS, se ha aprovechado la variedad de fuentes de datos disponibles. Esto permite obtener información diversa y enriquecedora de diferentes organismos y fuentes confiables, lo que a su vez mejora la calidad y la relevancia de los datos extraídos.

Te presentamos el proceso de obtención y extracción de datos, donde utilizamos tanto fuentes estáticas como dinámicas.

### Base de datos estática (.CSV)

Desde la página del Banco Mundial, realizamos una selección de los indicadores relevantes para nuestro análisis. Aplicamos filtros según los países, años y series de datos que nos interesan, para obtener una información más específica y precisa. Una vez configurados los parámetros de búsqueda, procedemos a descargar los datos en formato CSV.


### Base de datos dinámica (API)

Paso a detallar descarga descarga de la API Organización Mundial de la Salud(WHO):

Se pide respuesta a la url donde se encuentra el archivo con el indicador pedido. Se realiza la descarga. Se lee el archivo y se crea un nuevo archivo para manipulación con la biblioteca pyspark. Esto se debe hacer indicador por indicador. 
Luego se realiza la unión de los indicadores descargados en un solo archivo .csv.

---
## Diseño adecuado del Modelo ER:

Para las estructuras de almacenamiento de datos, adoptaremos un modelo entidad-relación (ER). Diseñaremos y especificaremos las tablas, relaciones y tipos de datos necesarios para representar de manera adecuada los datos relacionados con la esperanza de vida al nacer.

---
## Pipelines para alimentar el Data Lake:

Desarrollaremos pipelines de extracción, transformación y carga (ETL) para alimentar el Data Lake. Estos pipelines automatizan el proceso de integración y limpieza de los datos, asegurando la calidad de los mismos.

El primer paso será establecer la conexión con nuestro Data Lake en Azure Storage Blob utilizando las bibliotecas de Azure Storage Blobs Client. Esta conexión nos permitirá acceder y manipular los datos almacenados en el Data Lake.

A continuación, importamos los datos en formato “CSV”.

Luego aplicaremos las transformaciones necesarias para adaptarlos a nuestros requisitos. Esto incluirá procesos como limpieza de datos, filtrado, agregación y normalización. Utilizamos la biblioteca pandas para llevar a cabo estas transformaciones de manera eficiente.

Posterior a las transformaciones, procederemos a cargar los datos en nuestro Data Lake utilizando las bibliotecas de Azure Storage Blobs Client. Aseguraremos una estructura adecuada de carpetas y archivos que se ajuste a nuestras necesidades y al esquema de datos definido para nuestro Data Lake.

---
## Data Warehouse:

Implementaremos una estructura de Data Warehouse para almacenar los datos preparados y listos para su análisis. Para automatizar el proceso de carga y creación de tablas en una base de datos en Azure Synapse Analytics, utilizaremos Azure Data Factory.

Azure Data Factory es una herramienta de orquestación de datos en la nube, para crear un flujo de trabajo que nos permita extraer los datos de Azure Storage Blob y cargarlos en una base de datos en Azure Synapse Analytics.

Azure Data Factory nos brinda la capacidad de definir tareas de extracción, transformación y carga de datos (ETL) dentro de nuestro flujo de trabajo.

Una vez configurado el flujo de trabajo en Azure Data Factory, los datos se enviarán desde Azure Storage Blob a Azure Synapse Analytics. En este paso, se crearán las tablas en la base de datos de Azure Synapse Analytics

---
## Automatización:

La automatización de los procesos de ETL es una estrategia valiosa cuando se trata de manejar grandes volúmenes de datos que requieren actualizaciones frecuentes. Sin embargo, en el caso específico donde los datos se cargan con una frecuencia muy baja, no es necesario implementar una automatización completa por las siguientes razones:

`Frecuencia de carga:` Si los datos se cargan solo una vez al año o con una frecuencia muy baja, el esfuerzo y el tiempo necesario para desarrollar y mantener un sistema automatizado de ETL pueden superar los beneficios obtenidos. La automatización se vuelve más relevante cuando hay actualizaciones periódicas y regulares que justifican la inversión en tiempo y recursos para configurar y mantener los procesos automatizados.

`Complejidad de los procesos:` Los procesos de carga simples no involucran transformaciones complejas o una gran cantidad de datos, realizarlos localmente puede ser más eficiente y práctico.

`Costo y recursos:` La implementación de una automatización completa de los procesos de ETL implica invertir en herramientas, infraestructura y recursos humanos. Si los datos se cargan con baja frecuencia, destinar recursos significativos para desarrollar y mantener la automatización puede no ser justificado desde una perspectiva de costos.




# VER

## Información contiene este repositorio 

◌ Dentro de las carpetas "Sprint Semanal" encontrarán:

    Bitácora diaria detallando integramente el trabajo diario requerido por el proceso desde el primer hasta el último día.
    El apoyo visual para las presentaciones de final de Sprint ante el PO.
    El reporte oficial sobre los requerimientos del Sprint.
    Entregables específicos del Sprint.

◌ Dentro de la carpeta EDA, se encuentran:

    Todas las bases de datos utilizadas para el proceso, en formato .csv.
    El código de todo el proceso de Análisis Exploratorio de Datos, comentado y detallando paso a paso el proceso.

◌ En la carpeta entregable final, se encuentra:

    El reporte oficial, que contiene toda la información completa y detallada del workflow, el ciclo de vida del dato y los hallazgos producto de nuestros análisis.
    Código del front de nuestra URL.
    El apoyo visual para la presentación final.



























☄️ Licencia ☄️
El uso de este trabajo está licenciado bajo GNU General Public License v3.0 (GNU GPLv3).